name: Full Checks

on:
  push:
    branches:
      - main
      - master
      - dev
      - test
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

defaults:
  run:
    shell: bash

jobs:
  ruff:
    name: Ruff
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/${{ github.repository_owner }}/viewer-ci:latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Run Ruff
        run: ./scripts/checks/run_ruff.sh

  pylint:
    name: Pylint
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/${{ github.repository_owner }}/viewer-ci:latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Run Pylint
        id: run_pylint
        run: ./scripts/ci/run_pylint_with_report.sh
      - name: Upload Pylint artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pylint-report
          path: pylint-report
          if-no-files-found: error

  mypy:
    name: Mypy
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/${{ github.repository_owner }}/viewer-ci:latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Run mypy
        run: ./scripts/checks/run_mypy.sh

  pydoclint:
    name: Pydoclint
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/${{ github.repository_owner }}/viewer-ci:latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Install Pydoclint
        run: python -m pip install --disable-pip-version-check "pydoclint>=0.5.0"
      - name: Generate Pydoclint report
        run: ./scripts/checks/run_pydoclint.sh --output-dir pydoclint-report --summary-file pydoclint-report/summary.md
      - name: Publish Pydoclint summary
        if: always()
        run: ./scripts/ci/publish_pydoclint_summary.sh
      - name: Upload Pydoclint artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pydoclint-report
          path: pydoclint-report
          if-no-files-found: error

  radon:
    name: Radon
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/${{ github.repository_owner }}/viewer-ci:latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Install Radon
        run: python -m pip install --disable-pip-version-check "radon>=6.0.1"
      - name: Generate Radon report
        run: ./scripts/checks/run_radon.sh --output-dir radon-report --summary-file radon-report/summary.md
      - name: Publish Radon summary
        if: always()
        run: ./scripts/ci/publish_radon_summary.sh
      - name: Upload Radon artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: radon-report
          path: radon-report
          if-no-files-found: error

  vulture:
    name: Vulture
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/${{ github.repository_owner }}/viewer-ci:latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Install Vulture
        run: python -m pip install --disable-pip-version-check "vulture>=2.0"
      - name: Generate Vulture report
        run: ./scripts/checks/run_vulture.sh --output-dir vulture-report --summary-file vulture-report/summary.md
      - name: Publish Vulture summary
        if: always()
        run: ./scripts/ci/publish_vulture_summary.sh
      - name: Upload Vulture artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: vulture-report
          path: vulture-report
          if-no-files-found: error

  python-smells:
    name: Python Smells
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/${{ github.repository_owner }}/viewer-ci:latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Install PyExamine
        run: ./scripts/ci/install_pyexamine.sh
      - name: Generate Python Smells report
        run: ./scripts/checks/run_python_smells.sh --output-dir python-smells-report --summary-file python-smells-report/summary.md
      - name: Publish Python Smells summary
        if: always()
        run: ./scripts/ci/publish_python_smells_summary.sh
      - name: Upload Python Smells artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: python-smells-report
          path: python-smells-report
          if-no-files-found: error

  shellcheck:
    name: ShellCheck
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Install ShellCheck
        run: |
          sudo apt-get update
          sudo apt-get install -y shellcheck
      - name: Run ShellCheck
        id: run_shellcheck
        run: ./scripts/ci/run_shellcheck_with_report.sh
      - name: Upload ShellCheck artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: shellcheck-report
          path: shellcheck-report
          if-no-files-found: error

  hadolint:
    name: Hadolint
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Install Hadolint
        run: ./scripts/ci/install_hadolint.sh
      - name: Lint Dockerfile
        id: run_hadolint
        run: ./scripts/ci/run_hadolint_with_report.sh
      - name: Upload Hadolint artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: hadolint-report
          path: hadolint-report
          if-no-files-found: error

  eslint:
    name: ESLint
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      - name: Install dependencies
        run: npm ci
      - name: Run ESLint
        run: |
          SKIP_NPM_INSTALL=1 ./scripts/checks/run_eslint.sh

  stylelint:
    name: Stylelint
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      - name: Install dependencies
        run: npm ci
      - name: Run Stylelint
        run: |
          SKIP_NPM_INSTALL=1 ./scripts/checks/run_stylelint.sh

  uncss:
    name: UNCSS
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      - name: Install dependencies
        run: npm ci
      - name: Run UNCSS check
        run: |
          SKIP_NPM_INSTALL=1 ./scripts/checks/run_uncss_check.sh

  test-index:
    name: Test Index Validation
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/${{ github.repository_owner }}/viewer-ci:latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Verify test index
        id: verify_test_index
        run: ./scripts/ci/verify_test_index_with_report.sh
      - name: Upload test index artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-index-report
          path: test-index-report
          if-no-files-found: error

  cid-validation:
    name: CID Validation
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/${{ github.repository_owner }}/viewer-ci:latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Validate CID files
        id: validate_cids
        run: ./scripts/ci/validate_cids_with_report.sh
      - name: Upload CID validation artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: cid-validation-report
          path: cid-validation-report
          if-no-files-found: error

  dead-fixtures:
    name: Dead Fixtures Check
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/${{ github.repository_owner }}/viewer-ci:latest
    env:
      DATABASE_URL: sqlite:////tmp/secureapp-ci.db
      SESSION_SECRET: test-secret-key
      TESTING: "True"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Install dependencies
        run: python -m pip install -r requirements.txt
      - name: Check for dead fixtures
        id: run_dead_fixtures
        run: ./scripts/ci/run_dead_fixtures_with_report.sh
      - name: Upload dead fixtures artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dead-fixtures-report
          path: dead-fixtures-report
          if-no-files-found: error

  unit-tests:
    name: Unit Tests and Coverage
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/${{ github.repository_owner }}/viewer-ci:latest
    env:
      DATABASE_URL: sqlite:////tmp/secureapp-ci.db
      SESSION_SECRET: test-secret-key
      TESTING: "True"
      HYPOTHESIS_PROFILE: ci
    outputs:
      test_result: ${{ steps.determine_results.outputs.test_result }}
      coverage_result: ${{ steps.determine_results.outputs.coverage_result }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Install dependencies
        run: python -m pip install -r requirements.txt
      - name: Cache testmon database
        uses: actions/cache@v4
        with:
          path: .testmondata
          key: testmon-${{ github.ref }}-${{ github.sha }}
          restore-keys: |
            testmon-${{ github.ref }}-
            testmon-
      - name: Run impacted tests first (testmon)
        id: run_impacted_tests
        run: ./scripts/checks/run_unit_tests_impacted.sh
        continue-on-error: true
      - name: Run full unit tests with coverage
        id: run_tests
        if: always()
        run: |
          TEST_STATUS=0
          python -m coverage run -m pytest \
            --ignore=tests/property \
            --junitxml=test-results.xml \
            --html=unit-test-report.html \
            --self-contained-html \
            || TEST_STATUS=$?
          echo "test_status=$TEST_STATUS" >> "$GITHUB_OUTPUT"
          exit 0
      - name: Check coverage threshold
        id: check_coverage
        if: always()
        run: |
          COVERAGE_STATUS=0
          python -m coverage report --fail-under=75 || COVERAGE_STATUS=$?
          echo "coverage_status=$COVERAGE_STATUS" >> "$GITHUB_OUTPUT"
          exit 0
      - name: Generate coverage reports
        id: coverage
        if: always()
        run: |
          python -m coverage html
          python -m coverage xml
          python -m coverage report > coverage-report.txt || true
          scripts/publish-coverage-summary.sh \
            coverage-report.txt \
            "${GITHUB_STEP_SUMMARY:-}" \
            "${GITHUB_OUTPUT:-}"
      - name: Determine final results
        id: determine_results
        if: always()
        run: |
          # Determine test result
          if [[ "${{ steps.run_impacted_tests.outcome }}" == "failure" ]] || [[ "${{ steps.run_tests.outputs.test_status }}" != "0" ]]; then
            echo "test_result=failure" >> "$GITHUB_OUTPUT"
          else
            echo "test_result=success" >> "$GITHUB_OUTPUT"
          fi
          # Determine coverage result
          if [[ "${{ steps.check_coverage.outputs.coverage_status }}" != "0" ]]; then
            echo "coverage_result=failure" >> "$GITHUB_OUTPUT"
          else
            echo "coverage_result=success" >> "$GITHUB_OUTPUT"
          fi
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-tests-results
          path: |
            test-results.xml
            unit-test-report.html
          if-no-files-found: warn
      - name: Upload coverage XML
        if: always() && steps.coverage.outputs.generated == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: coverage-xml
          path: coverage.xml
      - name: Upload HTML coverage report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-tests-html
          path: htmlcov
          if-no-files-found: warn
      - name: Fail if tests or coverage failed
        if: steps.determine_results.outputs.test_result == 'failure' || steps.determine_results.outputs.coverage_result == 'failure'
        run: exit 1

  db-equivalence:
    name: DB Equivalence Tests
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/${{ github.repository_owner }}/viewer-ci:latest
    env:
      DATABASE_URL: sqlite:////tmp/secureapp-ci.db
      SESSION_SECRET: test-secret-key
      TESTING: "True"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Install dependencies
        run: python -m pip install -r requirements.txt
      - name: Cache testmon database
        uses: actions/cache@v4
        with:
          path: .testmondata
          key: testmon-db-equivalence-${{ github.ref }}-${{ github.sha }}
          restore-keys: |
            testmon-db-equivalence-${{ github.ref }}-
            testmon-db-equivalence-
      - name: Run equivalence tests
        id: run_db_equivalence
        run: |
          set -o pipefail
          python run_equivalence_tests.py --junitxml=db-equivalence-report.xml | tee db-equivalence.log
        continue-on-error: true
      - name: Upload DB equivalence artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: db-equivalence-results
          path: |
            db-equivalence-report.xml
            db-equivalence.log
          if-no-files-found: warn
      - name: Fail if equivalence tests failed
        if: steps.run_db_equivalence.outcome == 'failure'
        run: exit 1

  hypothesis-tests:
    name: Hypothesis Tests
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/${{ github.repository_owner }}/viewer-ci:latest
    env:
      HYPOTHESIS_PROFILE: ci
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Install dependencies
        run: python -m pip install -r requirements.txt
      - name: Cache testmon database
        uses: actions/cache@v4
        with:
          path: .testmondata
          key: testmon-hypothesis-${{ github.ref }}-${{ github.sha }}
          restore-keys: |
            testmon-hypothesis-${{ github.ref }}-
            testmon-hypothesis-
      - name: Run hypothesis tests
        id: run_hypothesis_tests
        run: |
          set -o pipefail
          ./scripts/checks/run_property_tests.sh \
            --junitxml=hypothesis-tests-report.xml \
            | tee hypothesis-tests.log
        continue-on-error: true
      - name: Upload hypothesis test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: hypothesis-tests-results
          path: |
            hypothesis-tests-report.xml
            hypothesis-tests.log
          if-no-files-found: warn
      - name: Fail if hypothesis tests failed
        if: steps.run_hypothesis_tests.outcome == 'failure'
        run: exit 1

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/${{ github.repository_owner }}/viewer-ci:latest
    env:
      DATABASE_URL: sqlite:////tmp/secureapp-ci.db
      SESSION_SECRET: test-secret-key
      TESTING: "True"
      INTEGRATION_LOG_PATH: integration-tests.log
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Install dependencies
        run: python -m pip install -r requirements.txt
      - name: Cache testmon database
        uses: actions/cache@v4
        with:
          path: .testmondata
          key: testmon-integration-${{ github.ref }}-${{ github.sha }}
          restore-keys: |
            testmon-integration-${{ github.ref }}-
            testmon-integration-
      - name: Run integration tests
        id: run_integration
        run: |
          STATUS=0
          ./scripts/checks/run_integration_tests.sh --junitxml=integration-tests-report.xml || STATUS=$?
          echo "status=$STATUS" >> "$GITHUB_OUTPUT"
          exit 0
        shell: bash
        continue-on-error: true
      - name: Publish integration summary
        if: always()
        run: |
          python scripts/publish_integration_summary.py \
            --log "$INTEGRATION_LOG_PATH" \
            --junit integration-tests-report.xml
      - name: Upload integration test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-tests-results
          path: |
            integration-tests-report.xml
            ${{ env.INTEGRATION_LOG_PATH }}
      - name: Fail if integration tests failed
        if: steps.run_integration.outputs.status != '0'
        run: exit 1

  gauge-specs:
    name: Gauge Specs
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/${{ github.repository_owner }}/viewer-ci:latest
    env:
      DATABASE_URL: sqlite:////tmp/secureapp-ci.db
      SESSION_SECRET: test-secret-key
      TESTING: "True"
      USE_SYSTEM_GAUGE: "1"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Download Chromium for pyppeteer
        run: pyppeteer-install
      - name: Verify Chromium installation
        run: ./scripts/ci/verify_chromium_installation.sh
      - name: Check shared library dependencies
        run: ./scripts/ci/check_chromium_dependencies.sh
      - name: Check system resources
        run: ./scripts/ci/check_system_resources.sh
      - name: Test Chromium binary directly
        run: ./scripts/ci/test_chromium_binary.sh
      - name: Test browser launch with pyppeteer
        run: python3 scripts/test-browser-launch.py
      - name: Run Gauge specs
        id: run_gauge
        run: ./scripts/checks/run_gauge_specs.sh
        env:
          GAUGE_PYTHON_COMMAND: python3
          STEP_IMPL_DIR: "${{ github.workspace }}/step_impl"
          PYTHONPATH: "${{ github.workspace }}:${{ github.workspace }}/step_impl:${{ github.workspace }}/tests"
          GAUGE_LOG_FILE: "${{ github.workspace }}/reports/html-report/gauge-execution.log"
        continue-on-error: true
      - name: Publish Gauge summary
        if: always()
        run: |
          python scripts/publish_gauge_summary.py \
            --log "${{ github.workspace }}/reports/html-report/gauge-execution.log" \
            --html-report "${{ github.workspace }}/reports/html-report/index.html"
      - name: Generate Gauge failure report
        if: always()
        run: |
          python scripts/generate_gauge_failure_report.py \
            --log "${{ github.workspace }}/reports/html-report/gauge-execution.log" \
            --html-report "${{ github.workspace }}/reports/html-report/index.html" \
            --output "${{ github.workspace }}/reports/html-report/gauge-failures.md"
      - name: Capture Gauge debug log
        if: always()
        run: |
          if [ -f "${{ github.workspace }}/logs/gauge.log" ]; then
            cp "${{ github.workspace }}/logs/gauge.log" "${{ github.workspace }}/reports/html-report/gauge.log"
          fi
      - name: Upload Gauge HTML report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: gauge-html
          path: reports/html-report
          if-no-files-found: warn
      - name: Fail if Gauge specs failed
        if: steps.run_gauge.outcome == 'failure'
        run: exit 1

  ai-eval:
    name: AI Eval Tests
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/${{ github.repository_owner }}/viewer-ci:latest
    env:
      DATABASE_URL: sqlite:////tmp/secureapp-ci.db
      SESSION_SECRET: test-secret-key
      TESTING: "True"
      OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Install dependencies
        run: python -m pip install -r requirements.txt
      - name: Run AI eval tests
        id: run_ai_eval
        run: |
          if [ -z "$OPENROUTER_API_KEY" ]; then
            echo "OPENROUTER_API_KEY is not set, skipping AI eval tests"
            mkdir -p test-results
            echo "Skipped: OPENROUTER_API_KEY not configured" > test-results/ai-eval-skipped.txt
            exit 0
          fi
          ./run_ai_eval_tests.sh
        continue-on-error: true
      - name: Upload AI eval artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ai-eval-results
          path: test-results
          if-no-files-found: warn
      - name: Fail if AI eval tests failed
        if: steps.run_ai_eval.outcome == 'failure'
        run: exit 1

  deploy-reports:
    name: Publish Test Reports
    needs:
      - ruff
      - pylint
      - mypy
      - pydoclint
      - radon
      - vulture
      - python-smells
      - shellcheck
      - hadolint
      - eslint
      - stylelint
      - uncss
      - test-index
      - cid-validation
      - dead-fixtures
      - unit-tests
      - hypothesis-tests
      - integration-tests
      - gauge-specs
      - ai-eval
    if: ${{ always() && github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/dev' || github.ref == 'refs/heads/test') }}
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/${{ github.repository_owner }}/viewer-ci:latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Determine deployment path
        id: deployment_path
        run: |
          # Determine the branch-specific subdirectory
          if [ "${{ github.ref }}" = "refs/heads/dev" ]; then
            echo "branch_subdir=dev" >> "$GITHUB_OUTPUT"
            echo "public_base_url=https://curtcox.github.io/Viewer/dev" >> "$GITHUB_OUTPUT"
          elif [ "${{ github.ref }}" = "refs/heads/test" ]; then
            echo "branch_subdir=test" >> "$GITHUB_OUTPUT"
            echo "public_base_url=https://curtcox.github.io/Viewer/test" >> "$GITHUB_OUTPUT"
          else
            echo "branch_subdir=" >> "$GITHUB_OUTPUT"
            echo "public_base_url=https://curtcox.github.io/Viewer" >> "$GITHUB_OUTPUT"
          fi
      - name: Download existing GitHub Pages (for merging)
        continue-on-error: true
        run: |
          BRANCH_SUBDIR="${{ steps.deployment_path.outputs.branch_subdir }}"
          
          # Only download existing pages if we're deploying to a subdirectory
          if [ -n "$BRANCH_SUBDIR" ]; then
            echo "Downloading existing GitHub Pages content to merge with new reports..."
            
            # Download the current GitHub Pages site
            curl -L -o pages.tar.gz \
              "https://github.com/${{ github.repository }}/archive/gh-pages.tar.gz" \
              || echo "Could not download existing pages (this is normal for first deployment)"
            
            if [ -f pages.tar.gz ]; then
              mkdir -p existing-pages
              tar -xzf pages.tar.gz -C existing-pages --strip-components=1 || true
              
              # If we have existing content, use it as the base
              if [ -d existing-pages ]; then
                echo "Found existing pages content"
                mkdir -p site-base
                # Copy everything except the subdirectory we're about to update
                rsync -av --exclude="$BRANCH_SUBDIR" existing-pages/ site-base/ || true
              fi
            fi
          fi
      - name: Capture job statuses
        env:
          RUFF_STATUS: ${{ needs.ruff.result }}
          PYLINT_STATUS: ${{ needs.pylint.result }}
          MYPY_STATUS: ${{ needs.mypy.result }}
          PYDOCLINT_STATUS: ${{ needs.pydoclint.result }}
          RADON_STATUS: ${{ needs.radon.result }}
          VULTURE_STATUS: ${{ needs.vulture.result }}
          PYTHON_SMELLS_STATUS: ${{ needs.python-smells.result }}
          SHELLCHECK_STATUS: ${{ needs.shellcheck.result }}
          HADOLINT_STATUS: ${{ needs.hadolint.result }}
          ESLINT_STATUS: ${{ needs.eslint.result }}
          STYLELINT_STATUS: ${{ needs.stylelint.result }}
          UNCSS_STATUS: ${{ needs.uncss.result }}
          TEST_INDEX_STATUS: ${{ needs.test-index.result }}
          CID_VALIDATION_STATUS: ${{ needs['cid-validation'].result }}
          DEAD_FIXTURES_STATUS: ${{ needs.dead-fixtures.result }}
          UNIT_TESTS_RESULTS_STATUS: ${{ needs.unit-tests.outputs.test_result }}
          UNIT_TESTS_COVERAGE_STATUS: ${{ needs.unit-tests.outputs.coverage_result }}
          HYPOTHESIS_TESTS_STATUS: ${{ needs.hypothesis-tests.result }}
          INTEGRATION_TESTS_STATUS: ${{ needs.integration-tests.result }}
          GAUGE_SPECS_STATUS: ${{ needs.gauge-specs.result }}
          AI_EVAL_STATUS: ${{ needs.ai-eval.result }}
        run: ./scripts/ci/capture_job_statuses.sh
      - name: Download unit test results
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: unit-tests-results
          path: artifacts/unit-tests-results
      - name: Download unit test coverage report
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: unit-tests-html
          path: artifacts/unit-tests-coverage
      - name: Download Gauge report
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: gauge-html
          path: artifacts/gauge-specs
      - name: Download integration test results
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: integration-tests-results
          path: artifacts/integration-tests
      - name: Download hypothesis test results
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: hypothesis-tests-results
          path: artifacts/hypothesis-tests
      - name: Download Radon report
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: radon-report
          path: artifacts/radon
      - name: Download Vulture report
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: vulture-report
          path: artifacts/vulture
      - name: Download Python Smells report
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: python-smells-report
          path: artifacts/python-smells
      - name: Download Pylint report
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: pylint-report
          path: artifacts/pylint
      - name: Download Pydoclint report
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: pydoclint-report
          path: artifacts/pydoclint
      - name: Download ShellCheck report
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: shellcheck-report
          path: artifacts/shellcheck
      - name: Download Hadolint report
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: hadolint-report
          path: artifacts/hadolint
      - name: Download Test Index report
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: test-index-report
          path: artifacts/test-index
      - name: Download CID validation report
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: cid-validation-report
          path: artifacts/cid-validation
      - name: Download Dead Fixtures report
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: dead-fixtures-report
          path: artifacts/dead-fixtures
      - name: Download AI eval results
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: ai-eval-results
          path: artifacts/ai-eval
      - name: Prepare site content
        run: ./scripts/ci/prepare_report_site.sh
        env:
          PUBLIC_BASE_URL: ${{ steps.deployment_path.outputs.public_base_url }}
      - name: Organize site for branch deployment
        run: |
          BRANCH_SUBDIR="${{ steps.deployment_path.outputs.branch_subdir }}"
          
          if [ -n "$BRANCH_SUBDIR" ]; then
            echo "Organizing site for $BRANCH_SUBDIR subdirectory deployment..."
            
            # Start with existing pages as base if available
            if [ -d site-base ]; then
              echo "Merging with existing GitHub Pages content"
              mkdir -p site-final
              cp -r site-base/* site-final/ || true
            else
              echo "No existing pages found, creating fresh structure"
              mkdir -p site-final
            fi
            
            # Add the new branch-specific content
            mkdir -p site-final/$BRANCH_SUBDIR
            cp -r site/* site-final/$BRANCH_SUBDIR/
            
            # Replace site directory with merged content
            rm -rf site
            mv site-final site
          fi
          
          # Verify the structure
          echo "Final site structure:"
          find site -type f -name "*.html" | head -20
      - name: Upload artifact for GitHub Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: site
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
